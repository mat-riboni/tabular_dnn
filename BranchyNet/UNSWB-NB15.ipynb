{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2c1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_and_prepare_nb15\n",
    "from neural_network import *\n",
    "from dense_branchynet import *\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "numerical_cols = [\n",
    "    \"NUM_PKTS_128_TO_256_BYTES\",\n",
    "    \"RETRANSMITTED_OUT_PKTS\",\n",
    "    \"SRC_TO_DST_IAT_STDDEV\",\n",
    "    \"SRC_TO_DST_SECOND_BYTES\",\n",
    "    \"IN_PKTS\",\n",
    "    \"LONGEST_FLOW_PKT\",\n",
    "    \"NUM_PKTS_256_TO_512_BYTES\",\n",
    "    \"DST_TO_SRC_IAT_AVG\",\n",
    "    \"OUT_BYTES\",\n",
    "    \"NUM_PKTS_UP_TO_128_BYTES\",\n",
    "    \"DURATION_OUT\",\n",
    "    \"NUM_PKTS_512_TO_1024_BYTES\",\n",
    "    \"SRC_TO_DST_IAT_AVG\",\n",
    "    \"DURATION_IN\",\n",
    "    \"SHORTEST_FLOW_PKT\",\n",
    "    \"RETRANSMITTED_IN_PKTS\",\n",
    "    \"FLOW_DURATION_MILLISECONDS\",\n",
    "    \"IN_BYTES\",\n",
    "    \"MIN_IP_PKT_LEN\",\n",
    "    \"TCP_WIN_MAX_OUT\",\n",
    "    \"SRC_TO_DST_IAT_MIN\",\n",
    "    \"RETRANSMITTED_OUT_BYTES\",\n",
    "    \"DST_TO_SRC_IAT_MAX\",\n",
    "    \"DST_TO_SRC_SECOND_BYTES\",\n",
    "    \"DNS_TTL_ANSWER\",\n",
    "    \"NUM_PKTS_1024_TO_1514_BYTES\",\n",
    "    \"SRC_TO_DST_AVG_THROUGHPUT\",\n",
    "    \"DST_TO_SRC_IAT_STDDEV\",\n",
    "    \"OUT_PKTS\",\n",
    "    \"SRC_TO_DST_IAT_MAX\",\n",
    "    \"TCP_WIN_MAX_IN\",\n",
    "    \"MAX_IP_PKT_LEN\",\n",
    "    \"DST_TO_SRC_AVG_THROUGHPUT\",\n",
    "    \"DST_TO_SRC_IAT_MIN\",\n",
    "    \"RETRANSMITTED_IN_BYTES\"\n",
    "\n",
    "    ]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"PROTOCOL\",\n",
    "    \"L7_PROTO\",\n",
    "    \"TCP_FLAGS\",\n",
    "    \"CLIENT_TCP_FLAGS\",\n",
    "    \"SERVER_TCP_FLAGS\",\n",
    "    \"ICMP_TYPE\",\n",
    "    \"ICMP_IPV4_TYPE\",\n",
    "    \"DNS_QUERY_TYPE\",\n",
    "    \"FTP_COMMAND_RET_CODE\"\n",
    "    ]\n",
    "\n",
    "target_col = 'Attack'\n",
    "num_target_classes = 10\n",
    "dataset_path = 'datasets/NF-UNSW-NB15-v3.csv'\n",
    "batch_size = 2018\n",
    "epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df5c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader, cat_cardinalities, cw, target_names = load_and_prepare_nb15(\n",
    "    file_path=dataset_path,\n",
    "    target_col=target_col,\n",
    "    numerical_cols=numerical_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "embedding_dims = [min(50, (card + 1) // 2) for card in cat_cardinalities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "168e5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    hidden_layers_sizes=[256, 256, 256], \n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    embedding_dims=embedding_dims,\n",
    "    num_numerical_features=len(numerical_cols),\n",
    "    num_target_classes=num_target_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8610ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972d3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e31e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch: 0  |  Loss: 0.0408  |  F1 Score: 0.3490  |  Accuracy: 0.9869 ---\n",
      "--- Epoch: 1  |  Loss: 0.0383  |  F1 Score: 0.4018  |  Accuracy: 0.9873 ---\n",
      "--- Epoch: 2  |  Loss: 0.0369  |  F1 Score: 0.4563  |  Accuracy: 0.9876 ---\n",
      "--- Epoch: 3  |  Loss: 0.0362  |  F1 Score: 0.4586  |  Accuracy: 0.9879 ---\n",
      "--- Epoch: 4  |  Loss: 0.0359  |  F1 Score: 0.5191  |  Accuracy: 0.9881 ---\n",
      "--- Epoch: 5  |  Loss: 0.0355  |  F1 Score: 0.5313  |  Accuracy: 0.9882 ---\n",
      "--- Epoch: 6  |  Loss: 0.0349  |  F1 Score: 0.4677  |  Accuracy: 0.9885 ---\n",
      "--- Epoch: 7  |  Loss: 0.0344  |  F1 Score: 0.5492  |  Accuracy: 0.9886 ---\n",
      "--- Epoch: 8  |  Loss: 0.0341  |  F1 Score: 0.5571  |  Accuracy: 0.9887 ---\n",
      "--- Epoch: 9  |  Loss: 0.0338  |  F1 Score: 0.5570  |  Accuracy: 0.9887 ---\n",
      "--- Epoch: 10  |  Loss: 0.0338  |  F1 Score: 0.5499  |  Accuracy: 0.9888 ---\n",
      "--- Epoch: 11  |  Loss: 0.0339  |  F1 Score: 0.5697  |  Accuracy: 0.9889 ---\n",
      "--- Epoch: 12  |  Loss: 0.0336  |  F1 Score: 0.5715  |  Accuracy: 0.9890 ---\n",
      "--- Epoch: 13  |  Loss: 0.0338  |  F1 Score: 0.5639  |  Accuracy: 0.9890 ---\n",
      "--- Epoch: 14  |  Loss: 0.0339  |  F1 Score: 0.5700  |  Accuracy: 0.9889 ---\n",
      "--- Epoch: 15  |  Loss: 0.0339  |  F1 Score: 0.5863  |  Accuracy: 0.9890 ---\n",
      "--- Epoch: 16  |  Loss: 0.0341  |  F1 Score: 0.5620  |  Accuracy: 0.9890 ---\n",
      "--- Epoch: 17  |  Loss: 0.0341  |  F1 Score: 0.5773  |  Accuracy: 0.9890 ---\n",
      "--- Epoch: 18  |  Loss: 0.0342  |  F1 Score: 0.5665  |  Accuracy: 0.9889 ---\n",
      "--- Epoch: 19  |  Loss: 0.0342  |  F1 Score: 0.5780  |  Accuracy: 0.9890 ---\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=scheduler,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371bd972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model, wd=1e-4,\n",
    "                   lr_trunk=1e-3, lr_heads=2e-3, lr_emb=None):\n",
    "    if lr_emb is None:\n",
    "        lr_emb = lr_trunk * 0.5\n",
    "\n",
    "    trunk_params = list(model.fc1.parameters()) + \\\n",
    "                   list(model.fc2.parameters()) + \\\n",
    "                   list(model.fc3.parameters())\n",
    "    heads12_params = list(model.head1.parameters()) + list(model.head2.parameters())\n",
    "    head3_params   = list(model.head3.parameters())\n",
    "\n",
    "    opt = torch.optim.AdamW([\n",
    "        {\"params\": model.embeddings.parameters(), \"lr\": lr_emb},\n",
    "        {\"params\": trunk_params, \"lr\": lr_trunk},\n",
    "        {\"params\": heads12_params, \"lr\": lr_heads},\n",
    "        {\"params\": head3_params, \"lr\": lr_heads},\n",
    "    ], weight_decay=wd)\n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c23df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "branchynet = DenseBranchyNet(\n",
    "    hidden_layers_sizes=[256, 256, 256],\n",
    "    taus=[1.4, 1.6],\n",
    "    alphas=[0.2, 0.8, 0.9],\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    embedding_dims=embedding_dims,\n",
    "    num_numerical=len(numerical_cols),\n",
    "    num_target_classes=num_target_classes\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88ee0fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 001] train_loss=0.1480 (l1=0.1443 l2=0.0692 l3=0.0708)  | val_acc=0.9860  val_f1=0.9834\n",
      "[Ep 002] train_loss=0.0801 (l1=0.0552 l2=0.0409 l3=0.0404)  | val_acc=0.9868  val_f1=0.9847\n",
      "[Ep 003] train_loss=0.0748 (l1=0.0493 l2=0.0383 l3=0.0381)  | val_acc=0.9870  val_f1=0.9852\n",
      "[Ep 004] train_loss=0.0719 (l1=0.0462 l2=0.0370 l3=0.0368)  | val_acc=0.9872  val_f1=0.9853\n",
      "[Ep 005] train_loss=0.0699 (l1=0.0442 l2=0.0360 l3=0.0359)  | val_acc=0.9878  val_f1=0.9860\n",
      "[Ep 006] train_loss=0.0682 (l1=0.0429 l2=0.0352 l3=0.0350)  | val_acc=0.9878  val_f1=0.9860\n",
      "[Ep 007] train_loss=0.0669 (l1=0.0420 l2=0.0345 l3=0.0344)  | val_acc=0.9877  val_f1=0.9861\n",
      "[Ep 008] train_loss=0.0657 (l1=0.0412 l2=0.0339 l3=0.0337)  | val_acc=0.9882  val_f1=0.9868\n",
      "[Ep 009] train_loss=0.0648 (l1=0.0406 l2=0.0334 l3=0.0333)  | val_acc=0.9881  val_f1=0.9868\n",
      "[Ep 010] train_loss=0.0640 (l1=0.0402 l2=0.0330 l3=0.0328)  | val_acc=0.9883  val_f1=0.9868\n"
     ]
    }
   ],
   "source": [
    "branchynet.set_stage(\"stage0_trunk_final\")\n",
    "optimizer = make_optimizer(branchynet, lr_trunk=1e-3, lr_heads=2e-3, lr_emb=5e-4)\n",
    "hist0 = branchynet.fit(train_dataloader, valid_dataloader, optimizer, device, epochs=epochs//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766e2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 001] train_loss=0.0612 (l1=0.0343 l2=0.0323 l3=0.0316)  | val_acc=0.9885  val_f1=0.9871\n",
      "[Ep 002] train_loss=0.0607 (l1=0.0335 l2=0.0320 l3=0.0316)  | val_acc=0.9886  val_f1=0.9872\n",
      "[Ep 003] train_loss=0.0608 (l1=0.0334 l2=0.0320 l3=0.0317)  | val_acc=0.9885  val_f1=0.9872\n",
      "[Ep 004] train_loss=0.0607 (l1=0.0332 l2=0.0319 l3=0.0317)  | val_acc=0.9886  val_f1=0.9872\n",
      "[Ep 005] train_loss=0.0604 (l1=0.0331 l2=0.0317 l3=0.0316)  | val_acc=0.9886  val_f1=0.9873\n",
      "[Ep 006] train_loss=0.0607 (l1=0.0330 l2=0.0319 l3=0.0317)  | val_acc=0.9885  val_f1=0.9873\n",
      "[Ep 007] train_loss=0.0605 (l1=0.0330 l2=0.0318 l3=0.0316)  | val_acc=0.9886  val_f1=0.9873\n",
      "[Ep 008] train_loss=0.0596 (l1=0.0320 l2=0.0309 l3=0.0316)  | val_acc=0.9887  val_f1=0.9873\n",
      "[Ep 009] train_loss=0.0596 (l1=0.0320 l2=0.0309 l3=0.0316)  | val_acc=0.9886  val_f1=0.9873\n",
      "[Ep 010] train_loss=0.0596 (l1=0.0320 l2=0.0309 l3=0.0316)  | val_acc=0.9886  val_f1=0.9873\n"
     ]
    }
   ],
   "source": [
    "branchynet.set_stage(\"stage1_heads_only\")\n",
    "optimizer = make_optimizer(branchynet, lr_trunk=0.0, lr_heads=3e-3, lr_emb=0.0) \n",
    "hist1 = branchynet.fit(train_dataloader, valid_dataloader, optimizer, device, epochs=epochs//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c934d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 001] train_loss=0.0583 (l1=0.0314 l2=0.0305 l3=0.0307)  | val_acc=0.9888  val_f1=0.9876\n",
      "[Ep 002] train_loss=0.0575 (l1=0.0312 l2=0.0301 l3=0.0302)  | val_acc=0.9889  val_f1=0.9878\n",
      "[Ep 003] train_loss=0.0572 (l1=0.0312 l2=0.0300 l3=0.0300)  | val_acc=0.9889  val_f1=0.9879\n",
      "[Ep 004] train_loss=0.0568 (l1=0.0310 l2=0.0298 l3=0.0298)  | val_acc=0.9889  val_f1=0.9879\n",
      "[Ep 005] train_loss=0.0567 (l1=0.0310 l2=0.0297 l3=0.0297)  | val_acc=0.9889  val_f1=0.9879\n",
      "[Ep 006] train_loss=0.0563 (l1=0.0309 l2=0.0295 l3=0.0296)  | val_acc=0.9889  val_f1=0.9879\n",
      "[Ep 007] train_loss=0.0558 (l1=0.0306 l2=0.0292 l3=0.0292)  | val_acc=0.9890  val_f1=0.9879\n",
      "[Ep 008] train_loss=0.0556 (l1=0.0306 l2=0.0291 l3=0.0292)  | val_acc=0.9890  val_f1=0.9880\n",
      "[Ep 009] train_loss=0.0555 (l1=0.0305 l2=0.0291 l3=0.0291)  | val_acc=0.9890  val_f1=0.9879\n",
      "[Ep 010] train_loss=0.0554 (l1=0.0305 l2=0.0290 l3=0.0290)  | val_acc=0.9890  val_f1=0.9880\n"
     ]
    }
   ],
   "source": [
    "branchynet.set_stage(\"stage2_finetune_all\")\n",
    "optimizer = make_optimizer(branchynet, lr_trunk=1e-4, lr_heads=3e-4, lr_emb=5e-5)\n",
    "hist2 = branchynet.fit(train_dataloader, valid_dataloader, optimizer, device, epochs=epochs//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6a3940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def measure_stage_costs(model, loader, device, n_batches=5, warmup=1):\n",
    "    \"\"\"\n",
    "    Misura i tempi medi dei tre stadi:\n",
    "      c1   = embed + fc1 + head1\n",
    "      c2   = (fc2 + head2) incrementale (cioè oltre c1)\n",
    "      c3   = (fc3 + head3) incrementale (cioè oltre c1+c2)\n",
    "    Ritorna: dict {\"c1\": ..., \"c2\": ..., \"c3\": ...} in secondi.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Timer containers\n",
    "    t_c1, t_c12, t_c123 = [], [], []\n",
    "\n",
    "    # Limita n_batches a quello che c'è davvero\n",
    "    max_batches = min(n_batches + warmup, len(loader)) if hasattr(loader, \"__len__\") else n_batches + warmup\n",
    "    batches_done = 0\n",
    "\n",
    "    # Utility per timing\n",
    "    use_cuda = (device.type == \"cuda\")\n",
    "\n",
    "    if use_cuda:\n",
    "        starter = torch.cuda.Event(enable_timing=True)\n",
    "        ender   = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    for i, (x_num, x_cat, _) in enumerate(loader):\n",
    "        if batches_done >= max_batches:\n",
    "            break\n",
    "\n",
    "        x_num = x_num.to(device, non_blocking=True)\n",
    "        x_cat = x_cat.to(device, non_blocking=True).long()\n",
    "\n",
    "        # --- C1 ---\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "            starter.record()\n",
    "            x  = model._embed_input(x_num, x_cat)\n",
    "            h1 = F.relu(model.fc1(x)); l1 = model.head1(h1)\n",
    "            ender.record(); torch.cuda.synchronize()\n",
    "            c1 = starter.elapsed_time(ender) / 1000.0\n",
    "        else:\n",
    "            t0 = time.perf_counter()\n",
    "            x  = model._embed_input(x_num, x_cat)\n",
    "            h1 = F.relu(model.fc1(x)); l1 = model.head1(h1)\n",
    "            c1 = time.perf_counter() - t0\n",
    "\n",
    "        # --- C1 + C2 ---\n",
    "        if use_cuda:\n",
    "            starter.record()\n",
    "            h2 = F.relu(model.fc2(h1)); l2 = model.head2(h2)\n",
    "            ender.record(); torch.cuda.synchronize()\n",
    "            c12 = starter.elapsed_time(ender) / 1000.0 + c1\n",
    "        else:\n",
    "            t1 = time.perf_counter()\n",
    "            h2 = F.relu(model.fc2(h1)); l2 = model.head2(h2)\n",
    "            c12 = (time.perf_counter() - t1) + c1\n",
    "\n",
    "        # --- C1 + C2 + C3 ---\n",
    "        if use_cuda:\n",
    "            starter.record()\n",
    "            h3 = F.relu(model.fc3(h2)); l3 = model.head3(h3)\n",
    "            ender.record(); torch.cuda.synchronize()\n",
    "            c123 = starter.elapsed_time(ender) / 1000.0 + c12\n",
    "        else:\n",
    "            t2 = time.perf_counter()\n",
    "            h3 = F.relu(model.fc3(h2)); l3 = model.head3(h3)\n",
    "            c123 = (time.perf_counter() - t2) + c12\n",
    "\n",
    "        # Salta i warm-up\n",
    "        if i >= warmup:\n",
    "            t_c1.append(c1)\n",
    "            t_c12.append(c12)\n",
    "            t_c123.append(c123)\n",
    "\n",
    "        batches_done += 1\n",
    "\n",
    "    if len(t_c1) == 0:\n",
    "        raise RuntimeError(\"measure_stage_costs: nessun batch misurato (loader troppo corto?)\")\n",
    "\n",
    "    c1   = sum(t_c1)  / len(t_c1)\n",
    "    c12  = sum(t_c12) / len(t_c12)\n",
    "    c123 = sum(t_c123)/ len(t_c123)\n",
    "\n",
    "    # tempi incrementali\n",
    "    return {\"c1\": c1, \"c2\": max(1e-9, c12 - c1), \"c3\": max(1e-9, c123 - c12)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f932a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ: [37.200836181640625, 49.46376037597656] F1: 0.9879852739580331 rates: {'r1': 0.5, 'r2': 0.25564736127853394, 'r3': 0.24435263872146606} cost_norm: 0.6522680845615498\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def f1_baseline_head3(model, loader, device):\n",
    "    from sklearn.metrics import f1_score\n",
    "    model.eval(); y_true, y_pred = [], []\n",
    "    for x_num, x_cat, y in loader:\n",
    "        x_num = x_num.to(device); x_cat = x_cat.to(device).long()\n",
    "        _, _, out3 = model(x_num, x_cat)\n",
    "        y_true += y.tolist(); y_pred += out3.argmax(1).cpu().tolist()\n",
    "    return f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "use_margin = True  # stesso setting di evaluate/predict\n",
    "base_f1 = f1_baseline_head3(branchynet, valid_dataloader, device)\n",
    "cost = measure_stage_costs(branchynet, valid_dataloader, device, n_batches=5)\n",
    "\n",
    "best = branchynet.calibrate_taus(\n",
    "    valid_dataloader, device, use_margin=use_margin,\n",
    "    n_grid=21, mode=\"min_cost_at_f1\",\n",
    "    f1_min=base_f1 - 0.01,                 # es. max -1% drop F1\n",
    "    cost=cost\n",
    ")\n",
    "branchynet.taus = [best[\"t1\"], best[\"t2\"]]\n",
    "print(\"τ:\", branchynet.taus, \"F1:\", best[\"f1\"], \"rates:\", best[\"rates\"], \"cost_norm:\", best[\"cost\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "568201fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.cat([y for _, _, y in test_dataloader]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "759b5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_model = time.time()\n",
    "model_preds = model.predict(test_dataloader,device)\n",
    "end_model = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c2a5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_branchynet = time.time()\n",
    "branchy_preds = branchynet.predict(test_dataloader, device, early_exit=True, use_margin=use_margin)\n",
    "end_branchynet = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "155649ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic DNN: 4.830464 s\n",
      "BranchyNet:  4.041070 s\n"
     ]
    }
   ],
   "source": [
    "dur_model   = end_model - start_model\n",
    "dur_branchy = end_branchynet - start_branchynet\n",
    "\n",
    "print(f\"Classic DNN: {dur_model:.6f} s\")\n",
    "print(f\"BranchyNet:  {dur_branchy:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9116521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BranchyNet Overhead: -19.53%\n"
     ]
    }
   ],
   "source": [
    "overhead = 100 - ((100 / dur_branchy) * dur_model)\n",
    "print(f'BranchyNet Overhead: {overhead:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af7268be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report DNN===\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.2800    0.0380    0.0670       184\n",
      "      Backdoor     0.4688    0.0870    0.1468       517\n",
      "        Benign     1.0000    1.0000    1.0000    322654\n",
      "           DoS     0.5841    0.1649    0.2572       758\n",
      "      Exploits     0.7765    0.8032    0.7896      5823\n",
      "       Fuzzers     0.6879    0.9502    0.7980      3838\n",
      "       Generic     0.7357    0.6317    0.6797       714\n",
      "Reconnaissance     0.7424    0.5939    0.6599      1694\n",
      "     Shellcode     0.7652    0.4244    0.5459       238\n",
      "         Worms     0.5714    0.8000    0.6667        20\n",
      "\n",
      "      accuracy                         0.9889    336440\n",
      "     macro avg     0.6612    0.5493    0.5611    336440\n",
      "  weighted avg     0.9883    0.9889    0.9878    336440\n",
      "\n",
      "\n",
      "=== Classification Report BRANCHYNET===\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.3587    0.1793    0.2391       184\n",
      "      Backdoor     0.5054    0.0909    0.1541       517\n",
      "        Benign     1.0000    1.0000    1.0000    322654\n",
      "           DoS     0.5702    0.1715    0.2637       758\n",
      "      Exploits     0.7817    0.7999    0.7907      5823\n",
      "       Fuzzers     0.6866    0.9487    0.7966      3838\n",
      "       Generic     0.7470    0.6120    0.6728       714\n",
      "Reconnaissance     0.7371    0.5992    0.6610      1694\n",
      "     Shellcode     0.6992    0.3908    0.5013       238\n",
      "         Worms     0.5909    0.6500    0.6190        20\n",
      "\n",
      "      accuracy                         0.9889    336440\n",
      "     macro avg     0.6677    0.5442    0.5698    336440\n",
      "  weighted avg     0.9884    0.9889    0.9879    336440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Classification Report DNN===\")\n",
    "print(classification_report(y_true, model_preds.numpy(), target_names=target_names, digits=4))\n",
    "print(\"\\n=== Classification Report BRANCHYNET===\")\n",
    "print(classification_report(y_true, branchy_preds.numpy(), target_names=target_names, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
